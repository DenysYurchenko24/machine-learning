# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JaKYdQtSznfxTO_ofjpPEM39fOYH7rPJ

First and foremost, solving kaggle competitions differs from solving real problems in many ways, with probably the most significant one being that here you have the privilege to immediately observe your test score and thus modify your data preprocessing/algorithm/model accordingly so to make new submissions. 

Which is not usualy the case in real world.

A noticable part of this code is a good example of the mentioned issue, meaning that in some places general ML logic was substitued in favor of perfecting the test score (that I had access to along the way). Of course it's not the case of me completely manipulating the code to get the desired result, and overall the initial logic flow was preserved.

It is also worth mentioning that `titanic` dataset is a great field for data tweaking and experimenting, which implies that there are lots of different approaches to extract info from data, implement feature engineering, etc. The one presented here is obviously hugly incomplete and allows for various future manipulations (some of which are mentioned within the code).

This notebook got me a public score of `0.81339` (places 777-1007), which is infinitely far from perfect, but still pretty decent try considering the scoring of most notebooks I've looked into.
"""

from impyute.imputation.cs import mice

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import random
import os

#algos
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

from sklearn.metrics import accuracy_score, classification_report

random.seed(42) # global random seed to ensure complete reproducibility
np.random.seed(42)
os.environ['PYTHONHASHSEED'] = str(42)

df = pd.read_csv('train.csv') #train set
df_test = pd.read_csv('test.csv') #test set

print(df.shape) 
print(df.head())
print(df_test.shape) 
print(df_test.head())

#concatenate sets
concat = pd.concat([df, df_test], ignore_index=True)

print(concat.shape)
print(concat.tail())

"""# EDA"""

pd.set_option('display.max_columns', None) # display all df columns

print(concat.info())
print('_'*40)
print(concat.describe())

print('_'*40)
print('NaN values')
print('_'*40)
print(concat.isna().sum()) # all nans

print('_'*40)
print('Zero values')
print('_'*40)
print(concat[concat==0.].count()) # all zeros

"""Let's explore every feature step by step. Let's start with `Cabin` as most of the missing values are in there."""

print(concat.Cabin.unique())

print(concat.Cabin.str[0].unique())

"""Seems like we can extract deck (floor) value from there.

![Замещающий текст](https://upload.wikimedia.org/wikipedia/commons/8/84/Titanic_cutaway_diagram.png)
"""

cabins = ['A','B','C','D','E','F','G','T']

# let's find out the number of passengers on each deck, survival rate and 
# the most common Pclass for that deck

cab_df = pd.DataFrame(index=['Num_pas', 'Ratio', 'Pclass_mode'], dtype=float)

for i in cabins:
  cab_df[i]=[int(len(df[(df.Cabin.str[0] == i)])), 
             round(df.groupby([(df.Cabin.str[0] == i)])['Survived'].mean()[1],2), 
             int(df[(df.Cabin.str[0] == i)]['Pclass'].mode())]
                     
print(cab_df)

"""We can use `Pclass` mode to encode our cabins (A-E, T - 1st, F - 2nd, G - 3rd). As far as I know, this isn't completely accurate representation of class/deck classification (on real Titanic it was different), but we're working with data we have. Of course there's plenty of other info that could be extracted here, like people sharing same cabins or having several cabins attached to them, but we'll just go with the first letter for now.

Coming back to null-values, it seems uncommon that 17 passengers have zero `Fare`s (ticket prices). Let's elaborate.
"""

print(concat[concat['Fare']==0])

"""There's something wrong with Southampton embarkation point. We'll need to replace these zeros with the `Fare` modes for corresponding `Pclass`. Also, while we're here, let's take a look at a single NaN `Fare` value."""

print(concat[concat['Fare'].isna()])

"""We'll replace it with the mode for the 3rd class.

Next one is `Name`. Here we can use prefixes to create `Title` feature.
"""

print(concat.Name)
print('_'*40)

concat['Title'] = concat.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

print('Unique titles')
print('_'*40)
print(concat.Title.unique()) # unique titles

print('_'*40)
print('Unique titles count')
print('_'*40)
print(concat.Title.value_counts()) # number of unique titles

"""First 4 categories seem prevalent, so we'll merge them with the rest judging on the sex of the title (could've also dropped everything beyond `Master`, replacing with 'rare' or something, but preserving at least some info sounds good).

We'll also use `Parch` and `SibSp` features to create two new features reflecting the number of passenger's relatives, and impute missing values in `Age` and `Embarked`. So let's create our final transformation function.

# Feature engineering
"""

def transform_df(df):

  # merging titles towards top-4
  df.loc[(df['Title'] == 'Dr') & (df['Sex'] == 'female'), ['Title']] = 'Mrs'
  df['Title'] = df['Title'].replace(['Rev', 'Dr', 'Col', 'Major', 'Jonkheer',
                                     'Don', 'Sir','Capt'], 'Mr')
  df['Title'] = df['Title'].replace(['Mlle'], 'Miss')
  df['Title'] = df['Title'].replace(['Ms', 'Countess', 'Dona', 'Mme', 'Lady'], 
                                    'Mrs')

  title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4}

  df['Title'] = df['Title'].map(title_mapping)
  
  # converting sex variables
  df.loc[df.Sex == 'female', 'Sex'] = 0
  df.loc[df.Sex == 'male', 'Sex'] = 1

  # filling fare zeros as we discussed
  df.loc[(df['Fare']==0)&(df['Pclass']==1), ['Fare']] = int(df[df['Pclass']==1]['Fare'].mode())

  df.loc[(df['Fare']==0)&(df['Pclass']==2), ['Fare']] = int(df[df['Pclass']==2]['Fare'].mode())

  df.loc[(df['Fare']==0)&(df['Pclass']==3), ['Fare']] = int(df[df['Pclass']==3]['Fare'].mode())

  # filling single na value
  df.loc[df['Fare'].isna(), ['Fare']] = int(df[df['Pclass']==3]['Fare'].mode())
  
  # mapping cabins as we decided based on class. Leaving NaNs for now
  df.loc[df.Cabin.str[0] == 'A', 'Cabin'] = 1
  df.loc[df.Cabin.str[0] == 'B', 'Cabin'] = 1
  df.loc[df.Cabin.str[0] == 'C', 'Cabin'] = 1
  df.loc[df.Cabin.str[0] == 'D', 'Cabin'] = 1
  df.loc[df.Cabin.str[0] == 'E', 'Cabin'] = 1
  df.loc[df.Cabin.str[0] == 'F', 'Cabin'] = 2
  df.loc[df.Cabin.str[0] == 'G', 'Cabin'] = 3
  df.loc[df.Cabin.str[0] == 'T', 'Cabin'] = 1
  
  # filling 3 missing embarked values with 'S', which is the most common one 
  # (and also correct, judging on real Titanic info)
  df.Embarked.fillna('S', inplace=True)
  df.loc[df.Embarked == 'C', 'Embarked'] = 1
  df.loc[df.Embarked == 'Q', 'Embarked'] = 2
  df.loc[df.Embarked == 'S', 'Embarked'] = 3
  
  # using SibSp and Parch for feature engineering
  df['FamilySize'] = df['SibSp'] + df['Parch'] + 1 # count overall size of family
  df['IsAlone'] = 0
  df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1 # introduce new feature reflecting 
  # whether passenger was travelling by their own or had at least 1 relative

  # dropping redundant columns here to make df available for converting to float
  # for the next step. At this step you can try to include ticket values, group 
  # passengers using their family names from Name, and create feature 
  # reflecting fare per person, but we'll skip that
  df.drop(['Ticket', 'Name', 'PassengerId', 'Survived'], inplace=True, axis=1)
  df = df.astype(float)

  # age imputation using mice seems to work a little bit better than using
  # modes/medians, preserving more of an original data distribution
  imputed = mice(df.values)
  ages = imputed[:, 2]
  ages = [0 if age < 0 else age for age in ages] # choose only positive ages
  df['Age'] = ages

  return df.astype(float)

"""You've noticed that we haven't conducted any continuous variables encoding (no splitting into bins, etc). In that notebook we'll use just two tree-based classification algorithms, which are pretty indifferent to that type f transformations, scalers and PCA/t-SNE dimensionality reduction (all of which come handy in case of regression but don't really improve performance of `Random Forest` and `XGBClassifier`), so that will not be included.

We also haven't dropped any close-like features (say, `FamilySize` and `IsAlone`). We'll do this as a final step of data preprocessing after finishing missing values imputation and correlation analysis.

We will apply defined function to the concatenated dataset to preserve integrity of its possible statistical relations, and then we'll again divide it into train and test set.
"""

X_concat = transform_df(concat)

print(X_concat.shape)
print(X_concat.isna().sum())

"""Now let's deal with those 1014 `Cabin` missing values."""

print(X_concat.corr(method='spearman')['Cabin'])

"""Seems like we can try to recreate `Cabin` from `Pclass`, `Age` and `Fare` values. Note that hereinafter we use Spearman method to calculate correlaiton, making use of a more flexible apporach that allows to capture non-linear relations (unlike default Pearson method which relies on linearity)."""

imput_cab = X_concat.loc[:, ['Fare', 'Age', 'Pclass', 'Cabin']]

# preparing data for classifier
imput_cab_test = imput_cab[imput_cab['Cabin'].isna()] # choose rows with NaN Cabin
cab_Xtest = imput_cab_test.drop(columns='Cabin').to_numpy()

imput_cab_train = imput_cab.dropna()
cab_Xtrain = imput_cab_train.drop(columns='Cabin').to_numpy()

cab_y = imput_cab_train['Cabin'].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(cab_Xtrain, cab_y, test_size=0.2)

clf = RandomForestClassifier(n_estimators=700, max_features=2, max_depth=2, min_samples_leaf=2) # arbitrary hyperparameters
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

"""We got a good result because most of the cabins are ones (belonging to 1st class). Now let's impute values we got."""

missing_cab = clf.predict(cab_Xtest)
print(f'Predicted values: {len(missing_cab)}')
print('Missing values')
print(imput_cab.isna().sum())

indicies_of_missing = imput_cab[imput_cab['Cabin'].isna()].index

# filling in missing values
for fill_index, df_index in enumerate(indicies_of_missing):
    imput_cab.loc[df_index, 'Cabin'] = missing_cab[fill_index]


print(imput_cab) # imputed

print(X_concat[X_concat.index.isin(imput_cab.index)])
X_concat['Cabin'].update(imput_cab['Cabin']) # updating initial df with df of imputed values

print(X_concat) # df after imputation
print(X_concat.isna().sum())

"""# Feature selection"""

X = X_concat

sns.heatmap(X.corr(method='spearman').round(2), annot=True) # correlation between features
plt.show()

Xtrain = X.loc[:890,:]

correlated = pd.DataFrame({
    'Survived': [round(Xtrain[i].corr(df['Survived'], method='spearman'), 2) for i in Xtrain]
}, index=[i for i in Xtrain])

sns.heatmap(correlated, annot=True) # correlation between features and survived (for train set)
plt.show()

"""We'll eliminate `Age`, `SibSp`, `Parch` as the ones that have the lowest correlation with the result of survival. It is natural to exclude `SibSp` and `Parch `since we already have `FamilySize` and `IsAlone`, but eliminating `Age` seems to be a little counterintuitive - as well as leaving both `FamilySize` and `IsAlone`. Although it seems to lead to the best result when scalers are not used."""

X.drop(columns=['Age', 'SibSp', 'Parch'], inplace=True)

"""It's useful to understand how our feature engineering/selection influenced our data. In particular, we can spot which samples will create confusion for our ML model because of being encoded in the same way (so model can't distinguish between them) but having different labels. So let's see which passengers have different survival results but the same feature representation after data preprocessing."""

# passengers that didn't survive
Xtrain_nsur = Xtrain[df['Survived']==0]
Xtrain_nsur['Id'] = Xtrain_nsur.index+1
Xtrain_nsur.reset_index(drop=True,inplace=True)

# passengers that survived
Xtrain_sur = Xtrain[df['Survived']==1]
Xtrain_sur['Id'] = Xtrain_sur.index+1
Xtrain_sur.reset_index(drop=True,inplace=True)

# intersection from both classes - meaning model will see these passengers 
# identically while they actually had different outcomes (belong to different classes)
intersected_df = pd.merge(Xtrain_nsur, Xtrain_sur, how='inner', 
on=['Pclass', 'Sex','Fare', 'Cabin','Embarked','Title','FamilySize','IsAlone'])
intersected_df.drop_duplicates(subset=['Id_x'], keep='first', inplace=True)
intersected_df.drop_duplicates(subset=['Id_y'], keep='first', inplace=True)

pd.set_option('display.max_columns', None)

comp = intersected_df.rename(columns={'Id_x':'Not_survived',
                                     'Id_y':'Survived'})
print(comp)

# for future analysis
comp.to_excel('intersection.xlsx', index=False)
df.to_excel('initial_df.xlsx', index=False)

# now let's rudimentally print intersections from the initial df
i=0
for i in comp.index.to_numpy():
  data = []
  data.append(list(df.loc[comp.loc[i,:]['Not_survived']-1,:]))
  data.append(list(df.loc[comp.loc[i,:]['Survived']-1,:]))
  print(pd.DataFrame(data, columns=df.columns))
  print(' '*40)
  print('_'*40)

"""Most of the intersections are either differing on age (which is still not an important feature judging on the correlation heatmap) or there is space for variation because of the multiple NaNs. We can also see that some deaths/survivings seem to be completely random and driven by chance and not by some sort of rule or regularity we hope to derive using ML techniques. Say, there are plenty of random unfortunate events that could impede passengers id201 or id200 from surviving while letting passengers id82 and id346 survive. Recreating such circumstances in form of data engineering will take a lot of fantasy and time."""

# splitting datasets again and preparing data for ML models
Xtrain = X.loc[:890,:]
Xtest = X.loc[891:,:]

# universal recipe for surviving/dying on the Titanic
print(Xtrain[df['Survived']==0].mode(axis=0)) # most common deadly values
print(Xtrain[df['Survived']==1].mode(axis=0)) # most common survival values

Xtrain = Xtrain.to_numpy()
Xtest = Xtest.to_numpy()

train_labels_df = df['Survived']
train_labels = train_labels_df.to_numpy()
y = train_labels.flatten()

print(Xtrain.shape)
print(Xtest.shape)

"""It can be seen that in order to survive you have to be a woman, occupying first class cabin and have 1 relative (probably a sibling, because of `Title`=2, which corresponds to 'Miss'). Of course these values can be (and most likely are) highly biased due to missing values imputing manner (shiffting towards most common patterns as a result of using modes). You can explore more features that are vital for survival during previous steps, before some columns have been dropped.

# Prediction
"""

from sklearn.model_selection import cross_validate

# cross-validate with manually chosen hyperparameters 
# some of them increase performance, others prevent overfitting
clf = XGBClassifier(n_estimators=700, max_features=3, max_depth=3, min_samples_leaf=2, 
                     min_child_weight=10, subsample=0.4, colsample_bytree=0.4, 
                     min_samples_split=10, learning_rate=0.02) 

rfe_results = cross_validate(clf, Xtrain, y, cv  = 10, return_train_score=True)

print('Train XGB: ', rfe_results['train_score'].mean()*100)
print('Valid XGB: ', rfe_results['test_score'].mean()*100)

clf.fit(Xtrain, y)
pred = clf.predict(Xtest)

# make submission
submissionXGB = pd.DataFrame({'PassengerId': df_test['PassengerId'],
                           'Survived': pred})
submissionXGB.to_csv('sumbissionXGB.csv', index=False)

"""Final notes. As I've mentioned at the beginning, this version of the notebook is essentialy an attempt to coordinate problem solving with pre-known desired result. Because of that a lot of methods I've implemented along the way of tryng to crack the `titanic` didn't get into this version. Some of these methods are:


1.   Obviously, using wide range of ML algorithms, including:

> `Decision Tree`

> `Random Forest Regresor` & `XGBRegressor`

> `Logistic Regression`

> `Support Vector Machines Classifier`

> `Linear Discriminant Analysis`

> `Gaussian Naive Bayes`

> Clustering passengers with `KNN`

> `SGDClassifier`

> `Gradient Boosting Classifier`

> Vanilla Neural Network with 3 linear layers

These methods ended up giving 75-79% accuracy on public score.


2.   `GridSearchCV` for hyperparameters tuning (incl. those for dealing with imbalanced dataset like this one) and feature selection using in-built sklearn methods. The problem here is that `titanic` dataset is highly prone to overfitting and since train/validation score&their ratio meant very little when it came to the public score. It was logical that 83/82 on cross-validation gave better result on public test than 93/84, but why was 83/81 better than 83/82? In general, trying out different train-valid techniques (including 3-fold, 5-fold, 10-fold cross-validation, manually written cross-validation on randomly shuffled subsets, etc.) hasn't revealed valid connection between training/validation scores and eventual pblic test score on kaggle. So it was kind of acting blindly.


3. Applying sklearn scalers (`QuantileTransformer`, `PowerTransformer`, `RobustScaler`, `Normalizer`, `MaxAbsScaler`, `MinMaxScaler`, `StandardScaler`) to normalize/standartize the data before fitting the algorithm onto it or before implementing Principal Component Analysis (including merging highly correlated features into one or two components using PCA) or just in order to observe correlations.

4. Imputing missing values through sklearn imputers (`IterativeImputer`, `KNNImputer`). Also I've spent a good amount of time experimenting with defferent imputation methods - modes, medians, means (both for values across the set or for specific features only - like when we've imputed `Fare` values with modes of their corresponding `Pclass`es), and also external `mice` library. Creating intermediate ML algorithm and combining all these methods turned out to be a slightly better practice.

5. Splitting continuous features (`Fare`, `Age`) into 4-5 bins to do encoding. Since I ended up using tree-based model, it didn't really pay off.

My apologies for spelling/semantic mistakes or typos.
"""