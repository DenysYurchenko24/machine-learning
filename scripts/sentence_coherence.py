# -*- coding: utf-8 -*-
"""SentenceCoherence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MXZl_UosgSmOqxEbrdb5yvvBcCGGF0Nn

# Analyzing sentence coherence with LSTM+CNN neural net

This notebook proposes an algorithm to evaluate sentence-level coherence using developed from training corpus embedding matrix and LSTM-CNN deep neural network.
"""

import torch
from torch import nn, optim
import pandas as pd
import numpy as np
import re

"""Let's choose sentences of lenght 10 (words+punctuation) from English corpus of `Tatoeba` dataset."""

sentences = []
train_sen = []
length = []

sent = pd.read_csv('eng_sentences.tsv', sep="\t", header=None)

test_sent = [len(re.findall(r"[\w']+|[.,!?;]", i.lower())) for i in sent[2]]

from collections import Counter
data = Counter(test_sent)
print(data.most_common()) # most common lengths
print(np.mean(test_sent)) # average length

num_words=10

for i in sent[2]:
    splitsent = re.findall(r"[\w']+|[.,!?;]", i.lower()) # split into words and punc
    train_sen.append(splitsent)
    if len(splitsent)>=num_words:
        sentences.append(splitsent[:num_words]) # truncate if len>10

print(sentences[:10]) # first 10 sentences

"""Generating embedding matrix on train corpus using `gensim` library."""

from gensim.models import Word2Vec
from tqdm import tqdm

tqdm.pandas()

embedding_dim = 64

model = Word2Vec(sentences=train_sen, window=2, min_count=1,
                 sg=1, 
                 size=embedding_dim ,  
                 workers=4)

model.save("word2vec.model")

model.wv.most_similar('nice') # most similar words to 'nice' based on cosine similarity

import random

print(len(model.wv.vocab.keys())) # number of words in the embed matrix

vocabulary = []

for i in sentences:
  for y in i:
    if y not in vocabulary:
      vocabulary.append(y)

print(vocabulary[:20]) # words and punctuation from sentences of length 10
print('Number of words and punc from 10-length sentences: ', len(vocabulary))

# representing sentences of length 10as embeddings arrays
sent_embed = np.array([[model.wv[y] for y in i] for i in sentences]) 
print('Original', sent_embed.shape) # num of sent * num of words * embedding dim

swapped = sent_embed
sent_embed = sent_embed.copy()

for i in swapped:
  np.random.shuffle(i) # randomly shuffling words in each sentence to create non-coherent sentences

print('Swapped', swapped.shape)

labels_or = [1 for i in range(len(sent_embed))] # labels for coherent sentences
labels_sw = [0 for i in range(len(swapped))] # labels for non-coherent ones

# 1st sentence in form of embeddings
print(sent_embed[0])
print(swapped[0])

# 1st sentence in form of text
print(' '.join([model.wv.most_similar([i], topn=1)[0][0] for i in sent_embed[0]]))
print(' '.join([model.wv.most_similar([i], topn=1)[0][0] for i in swapped[0]]))

"""# Model

My neural net consists of a single bidirectional LSTM layer (technicaly, 2 layers) and a single 1-d convolutional layer with kernel size=2 which strides over pairs of words (hidden states of LSTM) with overlap=1, measuring coherence across words pairs. Conv layer is followed by two linear layers, making use of all the activation maps and resulting in 2 values (for non-coherent/coherent classes). LSTM dimension is 50, and number of conv filters is 30.
"""

batch_size = 100

class CohNet(nn.Module):
    def __init__(self,  hidden_size, cnn_size, num_words, embedding_dim):
        super(CohNet, self).__init__()
        self.hidden_size = hidden_size
        self.cnn_size = cnn_size
        self.num_words = num_words
        self.embedding_dim = embedding_dim
        
        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, 
                            bidirectional=True)  
        self.cnn = nn.Conv1d(hidden_size*2, cnn_size, kernel_size=2, stride=1)
        self.lin1 = nn.Linear((num_words-1)*cnn_size, 512)
        self.relu = nn.ReLU()
        self.lin2 = nn.Linear(512, 2)
        self.drop = nn.Dropout(.3)


    def forward(self, x, hidden):

      batch_size = x.size(0)
      out, hidden = self.lstm(x, hidden)
      out = self.drop(out)
      out = out.permute(0,2,1) # batch_size * hidden_dim * num_words
      # sliding over doubled hidden states
      repr = self.cnn(out) # feature maps
      
      x = repr.view(batch_size, -1)
      x = self.relu(self.lin1(x))
      x = self.lin2(x) # class preds

      return x, hidden, repr

    def initHidden(self, batch_size):
        weight = next(self.parameters()).data
        if torch.cuda.is_available():
          hidden = (weight.new(2, batch_size, self.hidden_size).zero_().cuda(),
                  weight.new(2, batch_size, self.hidden_size).zero_().cuda())
        else:
          hidden = (weight.new(2, batch_size, self.hidden_size).zero_().cpu(),
                  weight.new(2, batch_size, self.hidden_size).zero_().cpu())
        return hidden


hidden_size = 50
cnn_size = 30

net = CohNet(hidden_size, cnn_size, num_words, embedding_dim)
if torch.cuda.is_available(): net.cuda()

"""Splitting data"""

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

features = np.concatenate([sent_embed, swapped]) # all sentences

print(len(features))

labels = np.concatenate([labels_or,labels_sw])

print(len(labels)) # all labels

X_train, X_val_, y_train, y_val_ = train_test_split(features, labels, test_size=0.2, shuffle=True)
X_val, X_test, y_val, y_test = train_test_split(X_val_, y_val_, test_size = .0013, shuffle=True)

train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
val_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))
test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))

train_load = DataLoader(train_data, batch_size=100, shuffle=True, drop_last=True)
valid_load = DataLoader(val_data, batch_size=100, shuffle=True, drop_last=True)
test_load = DataLoader(test_data, batch_size=100, shuffle=True, drop_last=True)

print(len(train_load))
print(len(valid_load))
print(len(test_load))

torch.save(test_load, 'test_data.pth') # saving test loader, because Colab runs out of RAM

"""Training"""

from sklearn.metrics import accuracy_score

soft = nn.Softmax()
optimizer = optim.Adam(net.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

for e in range(10):
  hidden = net.initHidden(batch_size)
  train_sen = []
  train_res = []
  val_res = []
  train_loss = []
  val_loss = []
  train_lab = []
  val_lab = []
    
  for f,l in train_load:
    
    if torch.cuda.is_available():
      f, l = f.cuda(), l.cuda()
    
    hidden = tuple([e.data for e in hidden])
    net.zero_grad()
    out, hidden, _ = net(f, hidden)

    for i in out:
      train_res.append(int(torch.argmax(soft(i))))

    error = criterion(out, l)
    train_loss.append(error.item())

    for i in l.cpu().detach().numpy():
      train_lab.append(i)

    error.backward()
    optimizer.step()

  
  net.eval()

  valid_hidden_state = net.initHidden(batch_size)

  for f_val, l_val in valid_load:

    if torch.cuda.is_available():
      f_val, l_val = f_val.cuda(), l_val.cuda()
    
    valid_hidden_state = tuple([e.data for e in valid_hidden_state])
    out, valid_hidden_state, _ = net(f_val, valid_hidden_state)

    for i in out:
      val_res.append(int(torch.argmax(soft(i))))

    error = criterion(out, l_val)
    val_loss.append(error.item())

    for i in l_val.cpu().detach().numpy():
      val_lab.append(i)

  net.train()

  print(f'Epoch: {e+1}/10')
  print(f'Train loss: {np.mean(train_loss)}')
  print(f'Valid loss: {np.mean(val_loss)}')
  print(f'Training acc: {accuracy_score(train_lab, train_res)}')
  print(f'Val acc: {accuracy_score(val_lab, val_res)}')

"""After 10 epochs we got `98.5%` validation accuracy on ~`162k` sentences."""

torch.save(net.state_dict(), 'coherence_model.pt')

"""Changing runtime to cpu, redefining model class, reloading model params, test data, and embeddings."""

import torch
from torch import nn, optim
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from tqdm import tqdm

tqdm.pandas()

batch_size = 100

class CohNet(nn.Module):
    def __init__(self,  hidden_size, cnn_size, num_words, embedding_dim):
        super(CohNet, self).__init__()
        self.hidden_size = hidden_size
        self.cnn_size = cnn_size
        self.num_words = num_words
        self.embedding_dim = embedding_dim
        
        self.lstm = nn.LSTM(embedding_dim , hidden_size, batch_first=True, 
                            bidirectional=True)  
        self.cnn = nn.Conv1d(hidden_size*2, cnn_size, kernel_size=2, stride=1)
        self.lin1 = nn.Linear((num_words-1)*cnn_size, 512)
        self.relu = nn.ReLU()
        self.lin2 = nn.Linear(512, 2)
        self.drop = nn.Dropout(.3)


    def forward(self, x, hidden):

      batch_size = x.size(0)
      out, hidden = self.lstm(x, hidden)
      out = self.drop(out)
      out = out.permute(0,2,1)

      repr = self.cnn(out) # feature maps
      
      x = repr.view(batch_size, -1)
      x = self.relu(self.lin1(x))
      x = self.lin2(x) # class preds

      return x, hidden, repr

    def initHidden(self, batch_size):
        weight = next(self.parameters()).data
        if torch.cuda.is_available():
          hidden = (weight.new(2, batch_size, self.hidden_size).zero_().cuda(),
                  weight.new(2, batch_size, self.hidden_size).zero_().cuda())
        else:
          hidden = (weight.new(2, batch_size, self.hidden_size).zero_().cpu(),
                  weight.new(2, batch_size, self.hidden_size).zero_().cpu())
        return hidden


hidden_size = 50
cnn_size = 30
num_words = 10
embedding_dim = 64

device = torch.device('cpu')
net = CohNet(hidden_size, cnn_size, num_words, embedding_dim)
net.load_state_dict(torch.load('coherence_model.pt', map_location=device))
test_load = torch.load('test_data.pth')
net.eval()

model = Word2Vec.load("word2vec.model")

"""Testing on test data"""

from sklearn.metrics import accuracy_score

soft = nn.Softmax()

batch_size = 100
test_res = []
test_lab = []
sen_test = []
representations = []

hidden = net.initHidden(batch_size)

# turning senences to text
for f_test, l_test in test_load:
  for y in f_test:
    sen_test.append(' '.join([model.wv.most_similar([i], topn=1)[0][0] 
                              for i in y.numpy()]))

  out, hidden, repr = net(f_test, hidden)

  # actual score
  for i in l_test.cpu().detach().numpy():
      test_lab.append(i)
  
  # predicted score
  for i in out:
      test_res.append(soft(i))
  
  # feature maps
  for i in repr.cpu().detach().numpy():
    representations.append(i)

print(f'Test accuracy: '
      f'{accuracy_score(test_lab, [int(torch.argmax(i)) for i in test_res])}')

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

#30 filters of 9 word-to-word connections in the 1st sentence
print(representations[1])

print('Predicted: ', int(torch.argmax(test_res[1])))
print('Actual: ', test_lab[1])
print(sen_test[1])

"""Heatmap test results"""

import re

def res(numb):
  if numb == 1:
    return 'coherent'
  else: return 'non-coherent'
  
def show_representations(indx):
  print('Sentence:', sen_test[indx])
  print('Predicted: '+ res(int(torch.argmax(test_res[indx]))))
  print(f'Predicted coherence level: '
      f'{np.round(np.float(test_res[indx][1]*100), decimals=3)}%')
  print('Actual: '+res(test_lab[indx])+'\n')
  
  # average conv value for whole sentence for each filter
  hidden_mean = np.expand_dims(np.mean(representations[indx], axis=1),1)
  

  aver_maps = np.mean(hidden_mean)
  aver_maps = np.expand_dims(np.array([aver_maps for i in range(30)]),1)

  reps = []
  for i in zip(representations[indx], hidden_mean):
      reps.append(np.concatenate([i[0], i[1]]))

  reps = np.array(reps)
  # average conv value for each word pair by filter 
  # + final average of the sentences averages
  mn = [np.mean(reps, axis=0)]
  reps = np.append(reps, mn, axis=0)

  # derive words pairs for x label
  sentence = re.findall(r"[\w']+|[.,!?;]", sen_test[indx])
  words = [str(sentence[i:i+2][0])+' + '+str(sentence[i:i+2][-1]) 
           for i in range(0, len(sentence), 1)][:-1]

  fig, ax = plt.subplots(figsize=(21,11))
  x_axis = words+['overall average']
  y_axis = [i for i in range(1, 31)]+['average by filter']
  sns.set(font_scale=1.2)

  # multiply by 10 here for scaling
  plot = sns.heatmap(reps*10, vmin=-1, vmax=1, center=0, xticklabels=x_axis, 
                     yticklabels=y_axis, annot=True, linewidths=.3, 
                     cbar_kws={'label': 'coherence level'})
  plot.axes.set_title("Sentence Coherence Heatmap",fontsize=30)
  plot.set_ylabel("Filters",fontsize=25)
  plot.set_xlabel("Words",fontsize=25)
  plot.tick_params(labelsize=15)
  plot.figure.axes[-1].yaxis.label.set_size(20)
  plt.yticks(rotation=0) 
  plt.xticks(rotation=45)


show_representations(62)

"""Convolutional layer extracts features from samples by applying filters with learned weights representing the patterns that match those features. So higher values of the feature maps should point to feature match. In this particular case, higher values of convolutions mean higher level of word pair coherence. Final value (average across sentences and feature maps, `overall average`) represents overall level of sentence coherence as seen by convolutional layer. It's easy to notice that those sentences that were classified as coherent have positive overall average value, while those classified as non-coherent end up with negative value.

We can further investigate this heatmap and discover filters most responsible for correct classification, as well as words pairs that classifier has considered as the most coherent/non-coherent.

Naturally, coherent sentences are presented in more warm colors (pos values), while non-coherent sentences tend to cold ones (neg values).
"""

non_coh = []
coh = []

def aver_coherence():
  for i in representations:
    hidden_mean = np.mean(i, axis=1)*10
    aver_maps = np.mean(hidden_mean)
    indx = np.where(representations==i)[0][0]
    if test_lab[indx] == 0: 
      non_coh.append(aver_maps)
      if aver_maps>0: # if final value is positive for incoherent sent
        print(f'Sentence index {indx}: positive conv value for non-coherent sentence')
        print(sen_test[indx])
        print(f'Predicted coherence level: '
            f'{np.round(np.float(test_res[indx][1]*100), decimals=3)}%')
        print('Actual: '+res(test_lab[indx])+'\n')
        
    else:
      coh.append(aver_maps)
      if aver_maps<0: # if final value is negative for coherent sent
        print(f'Sentence index {indx}: negative conv calue for coherent sentenc')
        print(sen_test[indx])
        print(f'Predicted coherence level: '
            f'{np.round(np.float(test_res[indx][1]*100), decimals=3)}%')
        print('Actual: '+res(test_lab[indx])+'\n')

aver_coherence()

print('Conv values for non-coherent sentences: ', non_coh)
print('Conv values for coherent sentences: ', coh,'\n')
print('Number of non-coherent sentences: ', len(non_coh))
print('Number of coherent sentences: ', len(coh))
print('Average conv value for non-coherent sentences: ', np.mean(non_coh))
print('Average conv value for coherent sentences: ', np.mean(coh))

"""We can see that the average conv value of coherent sentences is `5.8` and corresponding value of non-coherent ones is `-9`. However, there are exceptions - negative values for coherent sentences and positive for non-coherent (sentence at index `97` was classified correctly nevertheless). Let's exlore those exceptions."""

show_representations(52)

"""Overall this sentence could be interpreted as coherent except for the last part,which doesn't mean much since a lot of sentences were truncated and hence aren't presented as fully coherent per se. Notice that classifier correctly debunks non-coherence within `. + no` word pair (value of `-7.7`), as it is unusual to have a period before next word."""

show_representations(116)

"""This one is obviously a classification fail, although final negative value is pretty close to zero, and some filters (`15,9,1,2,5`) did a good job classifying the sentence as coherent."""

show_representations(128)

"""Again, except an obvious miss at `. + it's` pair, most of words pairs are perfectly coherent, so it's understandable that model got confused.

# Final notes

Obviously, both model and training hyperparameters are arbitrary and might be changed for better performance, as well as model architecture in general - e.g. including learnable embedding layer (or using pre-trained word2vec/glove embeddings). But it seems like more essential is dataset size that simply didn't let the model learn which words pairs should be considered as coherent. So bigger and more diverse corpus should prevent mistakes like those above from happpening.

It's worth saying that the model shows better performance on longer sentences (initial length choice was `5` with loss rising about `4` times and ~`5%` drop in accuracy) and it would be interesting to investigate its behaviour on zero padded sentences.

As for use cases, described model can be modified to evaluate multiple-sentences text coherence (with only one hidden state derived from each sentence).
"""